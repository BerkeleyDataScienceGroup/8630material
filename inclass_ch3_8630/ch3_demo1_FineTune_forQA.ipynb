{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b0d16f",
   "metadata": {
    "id": "d5b0d16f"
   },
   "source": [
    "# Question Answering - fine tuning with OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e6fa9",
   "metadata": {
    "id": "d04e6fa9"
   },
   "source": [
    "How to fine-tune a GPT-3 model with Python API using your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c47ab",
   "metadata": {
    "id": "d60c47ab"
   },
   "source": [
    "Install `openai` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad279c3-fbb6-467c-a671-3b900d18f61e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cat '../requirements.txt' \n",
    "#%pip install -r '../requirements.txt' --quiet\n",
    "#%pip install openai --quiet\n",
    "#%pip install openai[datalib]<=0.28.1\n",
    "#pip install openai pandas\n",
    "#%conda install openai[datalib]\n",
    "#%pip install openai \"[datalib]\"\n",
    "#%pip install openai\"[datalib]\" --quiet\n",
    "#%pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407a7bc0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11690,
     "status": "ok",
     "timestamp": 1700665381520,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "407a7bc0",
    "outputId": "9b6c4e3c-56cf-4684-a6c9-b5f579714fc6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/3e/d3/309769dad11d5f75b81c7252d9dc849ed440d0921215e759af169054f3b6/openai-1.3.7-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.3.7-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/a2/65/6940eeb21dcb2953778a6895281c179efd9100463ff08cb6232bb6480da7/httpx-0.25.2-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Obtaining dependency information for pydantic<3,>=1.9.0 from https://files.pythonhosted.org/packages/0a/2b/64066de1c4cf3d4ed623beeb3bbf3f8d0cc26661f1e7d180ec5eb66b75a5/pydantic-2.5.2-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m984.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Collecting typing-extensions<5,>=4.5 (from openai)\n",
      "  Obtaining dependency information for typing-extensions<5,>=4.5 from https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for pydantic-core==2.14.5 from https://files.pythonhosted.org/packages/7c/f5/3e59681bd53955da311a7f4efbb6315d01006e9d18b8a06b527a22d3d923/pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Downloading openai-1.3.7-py3-none-any.whl (221 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: typing-extensions, h11, distro, annotated-types, pydantic-core, httpcore, pydantic, httpx, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.6.0 distro-1.8.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.7 pydantic-2.5.2 pydantic-core-2.14.5 typing-extensions-4.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afa4af",
   "metadata": {
    "id": "d3afa4af"
   },
   "source": [
    "Import necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56483e72",
   "metadata": {
    "id": "56483e72",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f669e",
   "metadata": {
    "id": "698f669e"
   },
   "source": [
    "# Define OpenAI API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcaff120",
   "metadata": {
    "id": "dcaff120",
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_key = \"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\"#\"YOUR_API_KEY\"\n",
    "client = OpenAI(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f08dd7",
   "metadata": {
    "id": "10f08dd7"
   },
   "source": [
    "# Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f0c1f",
   "metadata": {
    "id": "569f0c1f"
   },
   "source": [
    "**PROMPT**\n",
    "\n",
    "According to the [OpenAI API reference](https://beta.openai.com/docs/guides/fine-tuning \"fine-tuning reference\") you need to make sure to end each `prompt` with a suffix.\n",
    "\n",
    "You can use ` ->`.\n",
    "\n",
    "**COMPLETION**\n",
    "\n",
    "Also, make sure to end each `completion` with a suffix as well\n",
    "\n",
    "You can use `.\\n`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6485dbd",
   "metadata": {
    "id": "a6485dbd"
   },
   "source": [
    "Template:\n",
    "\n",
    "```\n",
    "data_file = [{\n",
    "    \"prompt\": \"Prompt ->\",\n",
    "    \"completion\": \" Ideal answer.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"Prompt ->\",\n",
    "    \"completion\": \" Ideal answer.\\n\"\n",
    "}]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5904b9",
   "metadata": {
    "id": "ca5904b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [{\n",
    "    \"prompt\": \"What is the capital of France? ->\",\n",
    "    \"completion\": \" Capital of France is Paris.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"What is the primary function of the heart? ->\",\n",
    "    \"completion\": \" Primary function of the heart is to pump blood throughout the body.\\n\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10cb45e",
   "metadata": {
    "id": "e10cb45e"
   },
   "source": [
    "# Save dict as JSONL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ed465",
   "metadata": {
    "id": "7c6ed465"
   },
   "source": [
    "Training data need to be a JSONL document.\n",
    "\n",
    "JSONL file is a newline-delimited JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4bc0cf",
   "metadata": {
    "id": "4e4bc0cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = \"training_data.jsonl\"\n",
    "\n",
    "with open(file_name, 'w') as output_file:\n",
    "    for entry in data:\n",
    "        json.dump(entry, output_file)\n",
    "        output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf169e",
   "metadata": {
    "id": "6cbf169e"
   },
   "source": [
    "# Check JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a5fe452",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1700665438245,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "7a5fe452",
    "outputId": "3d5bd181-4100-47bf-9055-9aa84c653c62",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_extras/pandas_proxy.py\", line 22, in __load__\n",
      "    import pandas\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/__init__.py\", line 48, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/api.py\", line 48, in <module>\n",
      "    from pandas.core.groupby import (\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.groupby.generic import (\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/generic.py\", line 70, in <module>\n",
      "    from pandas.core.frame import DataFrame\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py\", line 157, in <module>\n",
      "    from pandas.core.generic import NDFrame\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py\", line 152, in <module>\n",
      "    from pandas.core.window import (\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/window/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.window.ewm import (  # noqa:F401\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/window/ewm.py\", line 12, in <module>\n",
      "    import pandas._libs.window.aggregations as window_aggregations\n",
      "ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /opt/conda/lib/python3.10/site-packages/pandas/_libs/window/aggregations.cpython-310-x86_64-linux-gnu.so)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/openai\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/cli/_cli.py\", line 129, in main\n",
      "    _main()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/cli/_cli.py\", line 209, in _main\n",
      "    parsed.func(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/cli/_tools/fine_tunes.py\", line 49, in prepare_data\n",
      "    df, remediation = read_any_format(fname)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/lib/_validators.py\", line 493, in read_any_format\n",
      "    df = pd.read_json(fname, lines=True, dtype=str).fillna(\"\")  # type: ignore\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_utils/_proxy.py\", line 25, in __getattr__\n",
      "    proxied = self.__get_proxied__()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_utils/_proxy.py\", line 67, in __get_proxied__\n",
      "    self.__proxied = proxied = self.__load__()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_extras/pandas_proxy.py\", line 24, in __load__\n",
      "    raise MissingDependencyError(PANDAS_INSTRUCTIONS)\n",
      "openai._extras._common.MissingDependencyError: \n",
      "\n",
      "OpenAI error:\n",
      "\n",
      "    missing `pandas`\n",
      "\n",
      "This feature requires additional dependencies:\n",
      "\n",
      "    $ pip install openai[datalib]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## can't get this to work on sagemaker but this work without it.\n",
    "## this works on colab\n",
    "\n",
    "#!openai -k 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 tools fine_tunes.prepare_data -f training_data.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fed3c",
   "metadata": {
    "id": "fe2fed3c"
   },
   "source": [
    "# Upload file to your OpenAI account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa6f8267",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1220,
     "status": "ok",
     "timestamp": 1700665441599,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "fa6f8267",
    "outputId": "fbf6face-4f67-4b25-c1a0-c2b6ee92140a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-YlAPRUXswNzxUMGUIT1APvrc', bytes=244, created_at=1701978147, filename='training_data.jsonl', object='file', purpose='fine-tune', status='uploaded', status_details=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_response = client.files.create(\n",
    "  file=open(file_name, \"rb\"),\n",
    " purpose='fine-tune'\n",
    ")\n",
    "\n",
    "upload_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb4254",
   "metadata": {
    "id": "20fb4254"
   },
   "source": [
    "# Save file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93469f37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1700665443536,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "93469f37",
    "outputId": "b96c5561-5e6f-4691-8c87-d277d1ac7b18",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-YlAPRUXswNzxUMGUIT1APvrc'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_id = upload_response.id\n",
    "file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1007a6",
   "metadata": {
    "id": "3e1007a6"
   },
   "source": [
    "# Fine-tune a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6530b7",
   "metadata": {
    "id": "8d6530b7"
   },
   "source": [
    "**Announcement by OpenAI**\n",
    "\n",
    "\n",
    "On July 6, 2023, we announced the deprecation of ada, babbage, curie and davinci models. These models, including fine-tuned versions, will be turned off on January 4, 2024. We are actively working on enabling fine-tuning for upgraded base GPT-3 models as well as GPT-3.5 Turbo and GPT-4, we recommend waiting for those new options to be available rather than fine-tuning based off of the soon to be deprecated models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2da9b",
   "metadata": {
    "id": "57e2da9b"
   },
   "source": [
    "Every fine-tuning job starts from a base model, which defaults to **curie**.\n",
    "\n",
    "The choice of model influences both the performance of the model and the cost of running your fine-tuned model.\n",
    "\n",
    "Your model can be one of: **ada**, **babbage**, **curie**, or **davinci**.\n",
    "\n",
    "Visit [pricing page](https://openai.com/pricing#faq-fine-tuning-pricing-calculation) for details on fine-tune rates.\n",
    "\n",
    "\n",
    "The default model is **Curie**. If you'd like to use **DaVinci** instead, then add it as a base model to fine-tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16bb42a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1700665448513,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "16bb42a2",
    "outputId": "cff6446d-ab87-4be5-ec8e-78f7099fff73",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTune(id='ft-UnOhGDUIBoao7mh7WR2cUozB', created_at=1701978490, fine_tuned_model=None, hyperparams=Hyperparams(batch_size=None, learning_rate_multiplier=None, n_epochs=4, prompt_loss_weight=0.01, classification_n_classes=None, classification_positive_class=None, compute_classification_metrics=None), model='curie', object='fine-tune', organization_id='org-KapxrFRrKW5pvmmZ7tlJgsuI', result_files=[], status='pending', training_files=[FileObject(id='file-YlAPRUXswNzxUMGUIT1APvrc', bytes=244, created_at=1701978147, filename='training_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)], updated_at=1701978490, validation_files=[], events=[FineTuneEvent(created_at=1701978490, level='info', message='Created fine-tune: ft-UnOhGDUIBoao7mh7WR2cUozB', object='fine-tune-event')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_response = client.fine_tunes.create(training_file=file_id)\n",
    "fine_tune_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059b2b3",
   "metadata": {
    "id": "2059b2b3"
   },
   "source": [
    "# Check fine-tune progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fda90",
   "metadata": {
    "id": "cc7fda90"
   },
   "source": [
    "Check the progress and get a list of all the fine-tuning events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cf062ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1700666273921,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "4cf062ab",
    "outputId": "76fcef82-5ea9-40e8-a873-f3d0b8577a67",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuneEventsListResponse(data=[FineTuneEvent(created_at=1701978490, level='info', message='Created fine-tune: ft-UnOhGDUIBoao7mh7WR2cUozB', object='fine-tune-event')], object='list')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_events = client.fine_tunes.list_events(fine_tune_id=fine_tune_response.id)\n",
    "fine_tune_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "HJc6iRazsU9N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1700666277357,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "HJc6iRazsU9N",
    "outputId": "34741692-cff9-45ee-ff78-55883adcecb1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FineTuneEvent(created_at=1701978490, level='info', message='Created fine-tune: ft-UnOhGDUIBoao7mh7WR2cUozB', object='fine-tune-event')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_events.__dict__['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f5bf3",
   "metadata": {
    "id": "362f5bf3"
   },
   "source": [
    "Check the progress and get an object with the fine-tuning job data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f01831be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1700666896304,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "f01831be",
    "outputId": "24f8b1c4-699e-41d7-bce8-0a59eb7e0b03",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTune(id='ft-UnOhGDUIBoao7mh7WR2cUozB', created_at=1701978490, fine_tuned_model=None, hyperparams=Hyperparams(batch_size=None, learning_rate_multiplier=None, n_epochs=4, prompt_loss_weight=0.01, classification_n_classes=None, classification_positive_class=None, compute_classification_metrics=None), model='curie', object='fine-tune', organization_id='org-KapxrFRrKW5pvmmZ7tlJgsuI', result_files=[], status='pending', training_files=[FileObject(id='file-YlAPRUXswNzxUMGUIT1APvrc', bytes=244, created_at=1701978147, filename='training_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)], updated_at=1701978490, validation_files=[], events=[FineTuneEvent(created_at=1701978490, level='info', message='Created fine-tune: ft-UnOhGDUIBoao7mh7WR2cUozB', object='fine-tune-event')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_response = client.fine_tunes.retrieve(fine_tune_id=fine_tune_response.id)\n",
    "retrieve_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "FqW-k0EBs8DH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1700666283467,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "FqW-k0EBs8DH",
    "outputId": "0523eabc-fc8b-475f-a933-56cebaca39b6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'created_at', 'fine_tuned_model', 'hyperparams', 'model', 'object', 'organization_id', 'result_files', 'status', 'training_files', 'updated_at', 'validation_files', 'events'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'ft-UnOhGDUIBoao7mh7WR2cUozB',\n",
       " 'created_at': 1701978490,\n",
       " 'fine_tuned_model': None,\n",
       " 'hyperparams': Hyperparams(batch_size=None, learning_rate_multiplier=None, n_epochs=4, prompt_loss_weight=0.01, classification_n_classes=None, classification_positive_class=None, compute_classification_metrics=None),\n",
       " 'model': 'curie',\n",
       " 'object': 'fine-tune',\n",
       " 'organization_id': 'org-KapxrFRrKW5pvmmZ7tlJgsuI',\n",
       " 'result_files': [],\n",
       " 'status': 'pending',\n",
       " 'training_files': [FileObject(id='file-YlAPRUXswNzxUMGUIT1APvrc', bytes=244, created_at=1701978147, filename='training_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)],\n",
       " 'updated_at': 1701978490,\n",
       " 'validation_files': [],\n",
       " 'events': [FineTuneEvent(created_at=1701978490, level='info', message='Created fine-tune: ft-UnOhGDUIBoao7mh7WR2cUozB', object='fine-tune-event')]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(retrieve_response.__dict__.keys())\n",
    "retrieve_response.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729581a",
   "metadata": {
    "id": "9729581a"
   },
   "source": [
    "**Wait for the model to be fine-tune, it takes 15/20 minutes!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e846ed",
   "metadata": {
    "id": "07e846ed"
   },
   "source": [
    "# Save fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635655e",
   "metadata": {
    "id": "3635655e"
   },
   "source": [
    "If `fine_tune_response.fine_tuned_model == None:` you can get the **fine_tuned_model** by listing all fine-tune events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77d2f317",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1700666287355,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "77d2f317",
    "outputId": "bf7d5efe-fa70-4cf5-ba85-c9a82cf7cbdc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if fine_tune_response.fine_tuned_model == None:\n",
    "    #fine_tune_list = openai.FineTune.list()\n",
    "    fine_tune_list=client.fine_tunes.list()\n",
    "    fine_tune_dict=fine_tune_list.__dict__\n",
    "    #fine_tuned_model = fine_tune_dict['data'][0].fine_tuned_model\n",
    "\n",
    "#print(fine_tune_dict)\n",
    "#fine_tune_list.'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601df11",
   "metadata": {
    "id": "8601df11"
   },
   "source": [
    "# Test the new model on a new prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654268c",
   "metadata": {
    "id": "c654268c"
   },
   "source": [
    "Remember to end the prompt with the same suffix as we used in the training data; ` ->`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37cfb2da",
   "metadata": {
    "id": "37cfb2da",
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_prompt = \"Which country serves as the primary capital of Paris? ->\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bee69cb8",
   "metadata": {
    "id": "bee69cb8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = client.completions.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt,\n",
    "  max_tokens=10, # Change amount of tokens for longer completion\n",
    "  temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "RDq2cUhOyp4_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1700666295721,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "RDq2cUhOyp4_",
    "outputId": "f91b6bc5-33a9-444e-da1e-ad25f43a8804",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " France\n",
      "\n",
      "Which country serves as the primary capital\n"
     ]
    }
   ],
   "source": [
    "#answer['choices'][0]['text']\n",
    "print(dict(answer.choices[0])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df769747-1842-4e17-bb85-3c5268087460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "colab": {
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
