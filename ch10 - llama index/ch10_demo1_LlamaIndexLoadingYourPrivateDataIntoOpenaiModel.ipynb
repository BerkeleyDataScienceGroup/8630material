{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7764a54",
   "metadata": {
    "id": "a7764a54",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Demo - RAG with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf90779",
   "metadata": {
    "id": "dbf90779"
   },
   "source": [
    "Demo of how to use LlamaIndex for reading files from local into OpenAI model and ask questions related to you data. The purpose of this demo is to show how OpenAI model can not have information about your private data to be able to give answers. LlamaIndex can help to integrate and connect your private data with OpenAI models using RAG and different types of document indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a194b",
   "metadata": {
    "id": "615a194b"
   },
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7ffc6e",
   "metadata": {
    "id": "ac7ffc6e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.8 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index --quiet\n",
    "%pip install openai --quiet\n",
    "%pip install pip install docx2txt --quiet\n",
    "%pip install llama-index openai pypdf --quiet\n",
    "# using s3fs to get public bucket contents\n",
    "%pip install fs_s3fs --quiet\n",
    "%pip install s3fs --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i2uSonYeYjQ9",
   "metadata": {
    "id": "i2uSonYeYjQ9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# %pip install awscli --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafcd82b",
   "metadata": {
    "id": "cafcd82b"
   },
   "source": [
    "Set up the OpenAI API key as an environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64bc281d",
   "metadata": {
    "id": "64bc281d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import s3fs # so we can access public s3 buckets\n",
    "\n",
    "api_key = \"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\"\n",
    "#fix\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "#old\n",
    "#openai.api_key = api_key\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa68c6",
   "metadata": {
    "id": "78fa68c6"
   },
   "source": [
    "Download Private-Data locally using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a715dc4d",
   "metadata": {
    "id": "a715dc4d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-12 17:17:19--  https://btcampdata.s3.amazonaws.com/gen-ai-data/Private-Data.zip\n",
      "Resolving btcampdata.s3.amazonaws.com (btcampdata.s3.amazonaws.com)... 16.12.64.140, 52.219.232.28, 52.219.84.180, ...\n",
      "Connecting to btcampdata.s3.amazonaws.com (btcampdata.s3.amazonaws.com)|16.12.64.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 548959 (536K) [application/zip]\n",
      "Saving to: ‘Private-Data.zip’\n",
      "\n",
      "Private-Data.zip    100%[===================>] 536.09K  2.66MB/s    in 0.2s    \n",
      "\n",
      "2023-12-12 17:17:20 (2.66 MB/s) - ‘Private-Data.zip’ saved [548959/548959]\n",
      "\n",
      "Archive:  Private-Data.zip\n",
      "  inflating: __MACOSX/._Private-Data  \n",
      "  inflating: Private-Data/CV4.pdf    \n",
      "  inflating: __MACOSX/Private-Data/._CV4.pdf  \n",
      "  inflating: Private-Data/CV5.pdf    \n",
      "  inflating: __MACOSX/Private-Data/._CV5.pdf  \n",
      "  inflating: Private-Data/CV7.pdf    \n",
      "  inflating: __MACOSX/Private-Data/._CV7.pdf  \n",
      "  inflating: Private-Data/CV6.pdf    \n",
      "  inflating: __MACOSX/Private-Data/._CV6.pdf  \n",
      "  inflating: Private-Data/CV2.pdf    \n",
      "  inflating: __MACOSX/Private-Data/._CV2.pdf  \n",
      "  inflating: Private-Data/CV3.pdf    \n",
      "  inflating: __MACOSX/Private-Data/._CV3.pdf  \n",
      "  inflating: Private-Data/CV1.pdf    \n",
      "  inflating: __MACOSX/Private-Data/._CV1.pdf  \n",
      "  inflating: Private-Data/CV8.pdf    \n",
      "  inflating: __MACOSX/Private-Data/._CV8.pdf  \n",
      "  inflating: Private-Data/CV9.pdf    \n",
      "  inflating: __MACOSX/Private-Data/._CV9.pdf  \n",
      "  inflating: Private-Data/CV12.pdf   \n",
      "  inflating: __MACOSX/Private-Data/._CV12.pdf  \n",
      "  inflating: Private-Data/CV13.pdf   \n",
      "  inflating: __MACOSX/Private-Data/._CV13.pdf  \n",
      "  inflating: Private-Data/CV11.pdf   \n",
      "  inflating: __MACOSX/Private-Data/._CV11.pdf  \n",
      "  inflating: Private-Data/CV10.pdf   \n",
      "  inflating: __MACOSX/Private-Data/._CV10.pdf  \n"
     ]
    }
   ],
   "source": [
    "#old\n",
    "#! aws s3 cp s3://webage-genai-data/Private-Data/ Private-Data --recursive\n",
    "\n",
    "!wget https://btcampdata.s3.amazonaws.com/gen-ai-data/Private-Data.zip\n",
    "!unzip Private-Data.zip\n",
    "\n",
    "#!wget https://btcampdata.s3.amazonaws.com/Private-Data.zip\n",
    "#!unzip Private-Data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb6d33",
   "metadata": {
    "id": "0bdb6d33"
   },
   "source": [
    "## Check OpenAI model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecb53c",
   "metadata": {
    "id": "ffecb53c"
   },
   "source": [
    "Check how the OpenAI model would answer on some specific question related to our private database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92541acd",
   "metadata": {
    "id": "92541acd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(model=model,\n",
    "                                            messages=messages,\n",
    "                                            temperature=0)\n",
    "\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22576a6",
   "metadata": {
    "id": "e22576a6"
   },
   "source": [
    "Our question that we will ask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed20bf0e",
   "metadata": {
    "id": "ed20bf0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"For what companies did Susan work in the past?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a133020",
   "metadata": {
    "id": "3a133020",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"I'm sorry, but as an AI language model, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I can't provide information about Susan's past work experience unless it has been mentioned previously.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07ff9f",
   "metadata": {
    "id": "ba07ff9f"
   },
   "source": [
    "**We see that ChatGPT does not have information about Susan at all.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9fa27",
   "metadata": {
    "id": "3ee9fa27"
   },
   "source": [
    "## Connect custom data sources to your LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80efd0",
   "metadata": {
    "id": "5c80efd0"
   },
   "source": [
    "We can use RAG to load our own data and feed LLM with our documents as context.\n",
    "\n",
    "It comes with many ready-made readers for sources such as databases, Discord, Slack, Google Docs, Notion, GitHub reps etc.\n",
    "\n",
    "Full list can be found here: https://llamahub.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d2e5f",
   "metadata": {
    "id": "563d2e5f"
   },
   "source": [
    "**SimpleDirectoryReader**\n",
    "\n",
    "`SimpleDirectoryReader` is used for reading data locally.\n",
    "In order to use it, simply pass in a input directory or a list of files. In our **Private-Data** folder, we have couple of CVs pdf files.\n",
    "\n",
    "`SimpleDirectoryReader` will select the best file reader (either for csv, pdf etc) based on the file extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba1198",
   "metadata": {
    "id": "fdba1198"
   },
   "source": [
    "Load CV files using SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b18eff84",
   "metadata": {
    "id": "b18eff84",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import TreeIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('Private-Data').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a953503",
   "metadata": {
    "id": "2a953503"
   },
   "source": [
    "By default, LlamaIndex uses OpenAI GPT-3 **text-davinci-003** model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b33361",
   "metadata": {
    "id": "62b33361"
   },
   "source": [
    "Create **TreeIndex** using loaded documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4097a0ad",
   "metadata": {
    "id": "4097a0ad",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "index = TreeIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28451da0",
   "metadata": {
    "id": "28451da0"
   },
   "source": [
    "Creating query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "966d6526",
   "metadata": {
    "id": "966d6526",
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd08d8",
   "metadata": {
    "id": "06cd08d8"
   },
   "source": [
    "Getting response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16b81e8e",
   "metadata": {
    "id": "16b81e8e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUNA, LOS ANGELES\n"
     ]
    }
   ],
   "source": [
    "prompt = \"For what companies did Susan work in the past?\"\n",
    "response = query_engine.query(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf94ab",
   "metadata": {
    "id": "10bf94ab"
   },
   "source": [
    "Depending on the answer, but if the model did not know, we might try different type if indexing or changing OpenAI model. Or change prompt to be more specific.\n",
    "\n",
    "Let's try new OpenAI model firstly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a2211",
   "metadata": {
    "id": "877a2211"
   },
   "source": [
    "### Changing the underlying LLM / Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd2cd46",
   "metadata": {
    "id": "efd2cd46"
   },
   "source": [
    "Sometimes, you want to use some other LLM for indexing instead of the default one.\n",
    "\n",
    "In this example, we use **gpt-3.5-turbo** instead of **text-davinci-003**. Available models include, gpt-3.5-turbo-16k, gpt-4, gpt-4-32k, text-davinci-003, and text-davinci-002 and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49662a34",
   "metadata": {
    "id": "49662a34",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8966bc",
   "metadata": {
    "id": "1f8966bc"
   },
   "source": [
    "Define a new LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bd4e89f",
   "metadata": {
    "id": "8bd4e89f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-16k-0613\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dea4c1",
   "metadata": {
    "id": "45dea4c1"
   },
   "source": [
    "Create **contex_service**\n",
    "\n",
    "The ServiceContext is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex pipeline/application. You can use it to set the global configuration, as well as local configurations at specific parts of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "154968f8",
   "metadata": {
    "id": "154968f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7604ac5",
   "metadata": {
    "id": "c7604ac5"
   },
   "source": [
    "Create again **TreeIndex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "969707bc",
   "metadata": {
    "id": "969707bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import TreeIndex\n",
    "\n",
    "index = TreeIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3abc01",
   "metadata": {
    "id": "7f3abc01"
   },
   "source": [
    "Start query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6161705a",
   "metadata": {
    "id": "6161705a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24574a",
   "metadata": {
    "id": "bd24574a"
   },
   "source": [
    "Run the same prompt as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2b041ee",
   "metadata": {
    "id": "f2b041ee",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUNA, LOS ANGELES\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb39d3b",
   "metadata": {
    "id": "ceb39d3b"
   },
   "source": [
    "Let's try another type of indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bac38cb",
   "metadata": {
    "id": "7bac38cb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bbf5e9b",
   "metadata": {
    "id": "1bbf5e9b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susan worked for Luxury Car Center and Japan Car Center in the past.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc6223",
   "metadata": {
    "id": "84bc6223"
   },
   "source": [
    "If the model still does not know the answer, we can be more specific with the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6bc5a4f",
   "metadata": {
    "id": "a6bc5a4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"For what companies did Susan work in the past?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e0f3384",
   "metadata": {
    "id": "1e0f3384",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susan worked for Luxury Car Center and Japan Car Center in the past.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910f514",
   "metadata": {
    "id": "5910f514"
   },
   "source": [
    "### Try another prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3f1cca7",
   "metadata": {
    "id": "c3f1cca7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt2 = \"Give me two names for software engineering position?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a027b9c7",
   "metadata": {
    "id": "a027b9c7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthew Eliot and Christopher Morgan.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(prompt2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588761a1",
   "metadata": {
    "id": "588761a1"
   },
   "source": [
    "### Let's try another type of indexing to see the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605f19e",
   "metadata": {
    "id": "b605f19e"
   },
   "source": [
    "### GPTVectorStoreIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05c0b2",
   "metadata": {
    "id": "ca05c0b2"
   },
   "source": [
    "GPTVectorStoreIndex creates numerical vectors from the text using word embeddings and retrieves relevant documents based on the similarity of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a82e7b2",
   "metadata": {
    "id": "4a82e7b2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "index2 = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine2 = index2.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69ff7bb8",
   "metadata": {
    "id": "69ff7bb8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susan worked for Luxury Car Center and Japan Car Center in the past.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine2.query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6c12e4c",
   "metadata": {
    "id": "e6c12e4c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthew Eliot and Christopher Morgan.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine2.query(prompt2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d28e0f",
   "metadata": {
    "id": "86d28e0f"
   },
   "source": [
    "### GPTListIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0471eb",
   "metadata": {
    "id": "ae0471eb"
   },
   "source": [
    "The GPTListIndex index is perfect when you don’t have many documents. Instead of trying to find the relevant data, the index concatenates all chunks and sends them all to the LLM. If the resulting text is too long, the index splits the text and asks LLM to refine the answer.\n",
    "\n",
    "Since we do not have too much documents, let's see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55e3ce93",
   "metadata": {
    "id": "55e3ce93",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.indices.list import GPTListIndex\n",
    "\n",
    "index3 = GPTListIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine3 = index3.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24ad091b",
   "metadata": {
    "id": "24ad091b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susan worked as a Head Waiter at Momo Restaurant in New York from September 2017 to May 2019. She also worked as a Waitress at Si Italian Restaurant in New York from September 2015 to May 2017.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine3.query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee744e4a",
   "metadata": {
    "id": "ee744e4a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christopher Morgan and Matthew Eliot are two names for software engineering positions.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine3.query(prompt2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41bd9c",
   "metadata": {
    "id": "ef41bd9c"
   },
   "source": [
    "### GPTKeywordTableIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45036fef",
   "metadata": {
    "id": "45036fef"
   },
   "source": [
    "The GPTKeywordTableIndex implementation extracts the keywords from indexed nodes and uses them to find relevant documents. When we ask a question, first, the implementation will generate keywords from the question. Next, the index searches for the relevant documents and sends them to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec3303",
   "metadata": {
    "id": "75ec3303"
   },
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "Using Keyword Indexing, every node is sent to the LLM to generate keywords. Sending every document to an LLM skyrockets the cost of indexing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "280c3e77",
   "metadata": {
    "id": "280c3e77",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.indices.keyword_table import GPTKeywordTableIndex\n",
    "\n",
    "index4 = GPTKeywordTableIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine4 = index4.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a8d8e79e",
   "metadata": {
    "id": "a8d8e79e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momo Restaurant and Si Italian Restaurant.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine4.query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f5f95f6",
   "metadata": {
    "id": "7f5f95f6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Matthew Eliot and Luna Software', source_nodes=[NodeWithScore(node=TextNode(id_='24f95f17-9c9c-461b-b951-d08ebb5283bf', embedding=None, metadata={'page_label': '1', 'file_name': 'CV3.pdf', 'file_path': 'Private-Data/CV3.pdf', 'file_type': 'application/pdf', 'file_size': 39534, 'creation_date': '2023-12-12', 'last_modified_date': '2023-09-17', 'last_accessed_date': '1970-01-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ca3da78f-d650-4974-b27c-ff1e41ec519d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'CV3.pdf', 'file_path': 'Private-Data/CV3.pdf', 'file_type': 'application/pdf', 'file_size': 39534, 'creation_date': '2023-12-12', 'last_modified_date': '2023-09-17', 'last_accessed_date': '1970-01-01'}, hash='6b3f1d80898c5c9a7805a58f3dc1b44c20e4df6eab89fda214081de3a2a6faae'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='16562131-57f6-4ced-9096-c5b220acdc79', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'CV2.pdf', 'file_path': 'Private-Data/CV2.pdf', 'file_type': 'application/pdf', 'file_size': 43809, 'creation_date': '2023-12-12', 'last_modified_date': '2023-09-11', 'last_accessed_date': '1970-01-01'}, hash='ea5836b28d4183856412115b44d1b9f2da8286f45155634b8508fb0e8b632e18'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='44959232-9dc2-4d8b-9b43-ff28eafb98c8', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7d064dcf20bcda404dc9a116e745e7de13953a2f7a16076a70feb7c76f1637fd')}, hash='6b3f1d80898c5c9a7805a58f3dc1b44c20e4df6eab89fda214081de3a2a6faae', text='MATTHEW ELIOT\\n344 ELM STREET MADISON, SD 57042\\n+1 (970) 333-3833 # MATTHEW.ELIOT@MAIL.COM #\\nLINKEDIN.COM/MATTHEWELIOT\\nSummary\\nSoftware Engineer with experience in all levels of testing, including performance, \\nfunctional, integration, system, regression, and user acceptance testing. Supportive\\nand enthusiastic team player dedicated to streamlining processes and efficiently \\nresolving project issues.\\nSkill Highlights\\n\\uf0b7Agile/Scrum methodology\\n\\uf0b7Performance and scalability\\uf0b7Optimization\\n\\uf0b7API design\\nExperience\\nSoftware Engineer - 09/2015 to 05/2019\\nLuna Software, New York\\n\\uf0b7Investigation, design, and implement scalable applications for data \\nidentification, analysis, retrieval, and indexing.\\n\\uf0b7Software design and development while remaining concentrate on client \\nneeds.\\n\\uf0b7Cooperate diligently with other IT team members to plan, design, and \\ndevelop smart solutions.\\n\\uf0b7Estimate interface between hardware and software.\\n\\uf0b7Interface with business analysts, developers, and technical support to \\ndetermine optimal specifications.\\nJunior Software Engineer - 09/2014 to 09/2015\\nAdsPro Software, New York\\n\\uf0b7Consulted regularly with customers on project status, proposals and technical\\nissues.\\n\\uf0b7Transformed existing software to correct errors, upgrade interfaces, and \\nimprove efficiency.\\n\\uf0b7Cooperate diligently with other IT team members to plan, design, and \\ndevelop smart solutions.\\nEducation\\nBachelor of Science:  Software Engineering - 2014\\nColumbia University, NY', start_char_idx=0, end_char_idx=1460, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)], metadata={'24f95f17-9c9c-461b-b951-d08ebb5283bf': {'page_label': '1', 'file_name': 'CV3.pdf', 'file_path': 'Private-Data/CV3.pdf', 'file_type': 'application/pdf', 'file_size': 39534, 'creation_date': '2023-12-12', 'last_modified_date': '2023-09-17', 'last_accessed_date': '1970-01-01'}})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_engine4.query(prompt2)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe98afd",
   "metadata": {
    "id": "6fe98afd"
   },
   "source": [
    "### Saving and Loading indexed documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1631803",
   "metadata": {
    "id": "f1631803"
   },
   "source": [
    "By default, data is stored in-memory. To persist to disk (under ./storage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97ce1f3b",
   "metadata": {
    "id": "97ce1f3b"
   },
   "outputs": [],
   "source": [
    "index4.storage_context.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929bea4e",
   "metadata": {
    "id": "929bea4e"
   },
   "source": [
    "To reload from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c92de84",
   "metadata": {
    "id": "4c92de84"
   },
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "# load index\n",
    "loaded_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10932ac2",
   "metadata": {
    "id": "10932ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthew Eliot and Luna Software\n"
     ]
    }
   ],
   "source": [
    "query_engine_loaded = loaded_index.as_query_engine()\n",
    "\n",
    "\n",
    "response_loaded_index = query_engine_loaded.query(prompt2)\n",
    "print(response_loaded_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TfMpDxfUU33-",
   "metadata": {
    "id": "TfMpDxfUU33-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a24e30-a992-4685-91d6-f61b0f0d9ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
