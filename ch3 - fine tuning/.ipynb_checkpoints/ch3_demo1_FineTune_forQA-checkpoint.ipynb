{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b0d16f",
   "metadata": {
    "id": "d5b0d16f"
   },
   "source": [
    "# Question Answering - fine tuning with OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e6fa9",
   "metadata": {
    "id": "d04e6fa9"
   },
   "source": [
    "How to fine-tune a GPT-3 model with Python API using your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c47ab",
   "metadata": {
    "id": "d60c47ab"
   },
   "source": [
    "Install `openai` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ad279c3-fbb6-467c-a671-3b900d18f61e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai<=0.28.1\n",
      "# wikipedia<1.3.1\n",
      "arxiv<=1.4.8\n",
      "xmltodict<=0.13.0\n",
      "boto3<=1.28.65\n",
      "numpy<=1.26.1\n",
      "matplotlib<=3.8.0\n",
      "pandas<=2.1.1\n",
      "scikit-learn<=1.3.1\n",
      "opendatasets<=0.1.22\n",
      "python-dotenv<=1.0.0\n",
      "accelerate>=0.16.0,<1\n",
      "transformers<=4.34.0\n",
      "transformers[torch]>=4.28.1,<5\n",
      "torch>=1.13.1,<2\n",
      "langchain>=0.0.139\n",
      "llama-index<=0.8.45\n",
      "docx2txt<=0.8\n",
      "pypdf<=3.16.4\n",
      "requests<=2.31.0"
     ]
    }
   ],
   "source": [
    "!cat '../requirements.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "407a7bc0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11690,
     "status": "ok",
     "timestamp": 1700665381520,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "407a7bc0",
    "outputId": "9b6c4e3c-56cf-4684-a6c9-b5f579714fc6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "\n",
      "ResolvePackageNotFound: \n",
      "  - conda=22.9.0\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install -r '../requirements.txt' --quiet\n",
    "#%pip install openai --quiet\n",
    "#%pip install openai[datalib]<=0.28.1\n",
    "#pip install openai pandas\n",
    "%conda install openai[datalib]\n",
    "#%pip install openai \n",
    "#%pip install openai \"[datalib]\"\n",
    "#%pip install openai\"[datalib]\" --quiet\n",
    "#%pip3 install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afa4af",
   "metadata": {
    "id": "d3afa4af"
   },
   "source": [
    "Import necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56483e72",
   "metadata": {
    "id": "56483e72",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f669e",
   "metadata": {
    "id": "698f669e"
   },
   "source": [
    "# Define OpenAI API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcaff120",
   "metadata": {
    "id": "dcaff120",
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_key = \"\"#\"YOUR_API_KEY\"\n",
    "#old\n",
    "#openai.api_key = api_key\n",
    "client = OpenAI(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f08dd7",
   "metadata": {
    "id": "10f08dd7"
   },
   "source": [
    "# Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f0c1f",
   "metadata": {
    "id": "569f0c1f"
   },
   "source": [
    "**PROMPT**\n",
    "\n",
    "According to the [OpenAI API reference](https://beta.openai.com/docs/guides/fine-tuning \"fine-tuning reference\") you need to make sure to end each `prompt` with a suffix.\n",
    "\n",
    "You can use ` ->`.\n",
    "\n",
    "**COMPLETION**\n",
    "\n",
    "Also, make sure to end each `completion` with a suffix as well\n",
    "\n",
    "You can use `.\\n`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6485dbd",
   "metadata": {
    "id": "a6485dbd"
   },
   "source": [
    "Template:\n",
    "\n",
    "```\n",
    "data_file = [{\n",
    "    \"prompt\": \"Prompt ->\",\n",
    "    \"completion\": \" Ideal answer.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"Prompt ->\",\n",
    "    \"completion\": \" Ideal answer.\\n\"\n",
    "}]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca5904b9",
   "metadata": {
    "id": "ca5904b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [{\n",
    "    \"prompt\": \"What is the capital of France? ->\",\n",
    "    \"completion\": \" Capital of France is Paris.\\n\"\n",
    "},{\n",
    "    \"prompt\":\"What is the primary function of the heart? ->\",\n",
    "    \"completion\": \" Primary function of the heart is to pump blood throughout the body.\\n\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10cb45e",
   "metadata": {
    "id": "e10cb45e"
   },
   "source": [
    "# Save dict as JSONL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ed465",
   "metadata": {
    "id": "7c6ed465"
   },
   "source": [
    "Training data need to be a JSONL document.\n",
    "\n",
    "JSONL file is a newline-delimited JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e4bc0cf",
   "metadata": {
    "id": "4e4bc0cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = \"training_data.jsonl\"\n",
    "\n",
    "with open(file_name, 'w') as output_file:\n",
    "    for entry in data:\n",
    "        json.dump(entry, output_file)\n",
    "        output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf169e",
   "metadata": {
    "id": "6cbf169e"
   },
   "source": [
    "# Check JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a5fe452",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1700665438245,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "7a5fe452",
    "outputId": "3d5bd181-4100-47bf-9055-9aa84c653c62",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## can't get this to work on sagemaker but this work without it.\n",
    "## this works on colab\n",
    "\n",
    "!openai -k {api_key} tools fine_tunes.prepare_data -f training_data.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fed3c",
   "metadata": {
    "id": "fe2fed3c"
   },
   "source": [
    "# Upload file to your OpenAI account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa6f8267",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1220,
     "status": "ok",
     "timestamp": 1700665441599,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "fa6f8267",
    "outputId": "fbf6face-4f67-4b25-c1a0-c2b6ee92140a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-kVYjbnur011YblP88xufg1Fu', bytes=244, created_at=1701381173, filename='training_data.jsonl', object='file', purpose='fine-tune', status='uploaded', status_details=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_response = client.files.create(\n",
    "  file=open(file_name, \"rb\"),\n",
    " purpose='fine-tune'\n",
    ")\n",
    "\n",
    "upload_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb4254",
   "metadata": {
    "id": "20fb4254"
   },
   "source": [
    "# Save file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93469f37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1700665443536,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "93469f37",
    "outputId": "b96c5561-5e6f-4691-8c87-d277d1ac7b18",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-kVYjbnur011YblP88xufg1Fu'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_id = upload_response.id\n",
    "file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1007a6",
   "metadata": {
    "id": "3e1007a6"
   },
   "source": [
    "# Fine-tune a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6530b7",
   "metadata": {
    "id": "8d6530b7"
   },
   "source": [
    "**Announcement by OpenAI**\n",
    "\n",
    "\n",
    "On July 6, 2023, we announced the deprecation of ada, babbage, curie and davinci models. These models, including fine-tuned versions, will be turned off on January 4, 2024. We are actively working on enabling fine-tuning for upgraded base GPT-3 models as well as GPT-3.5 Turbo and GPT-4, we recommend waiting for those new options to be available rather than fine-tuning based off of the soon to be deprecated models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2da9b",
   "metadata": {
    "id": "57e2da9b"
   },
   "source": [
    "Every fine-tuning job starts from a base model, which defaults to **curie**.\n",
    "\n",
    "The choice of model influences both the performance of the model and the cost of running your fine-tuned model.\n",
    "\n",
    "Your model can be one of: **ada**, **babbage**, **curie**, or **davinci**.\n",
    "\n",
    "Visit [pricing page](https://openai.com/pricing#faq-fine-tuning-pricing-calculation) for details on fine-tune rates.\n",
    "\n",
    "\n",
    "The default model is **Curie**. If you'd like to use **DaVinci** instead, then add it as a base model to fine-tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16bb42a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1700665448513,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "16bb42a2",
    "outputId": "cff6446d-ab87-4be5-ec8e-78f7099fff73",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTune(id='ft-DP03u7KFRxaaWEcFUPqTKaaq', created_at=1701381173, fine_tuned_model=None, hyperparams=Hyperparams(batch_size=None, learning_rate_multiplier=None, n_epochs=4, prompt_loss_weight=0.01, classification_n_classes=None, classification_positive_class=None, compute_classification_metrics=None), model='curie', object='fine-tune', organization_id='org-heTRxyuue7iTIuT3uUuUFCPN', result_files=[], status='pending', training_files=[FileObject(id='file-kVYjbnur011YblP88xufg1Fu', bytes=244, created_at=1701381173, filename='training_data.jsonl', object='file', purpose='fine-tune', status='uploaded', status_details=None)], updated_at=1701381173, validation_files=[], events=[FineTuneEvent(created_at=1701381173, level='info', message='Created fine-tune: ft-DP03u7KFRxaaWEcFUPqTKaaq', object='fine-tune-event')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_response = client.fine_tunes.create(training_file=file_id)\n",
    "fine_tune_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059b2b3",
   "metadata": {
    "id": "2059b2b3"
   },
   "source": [
    "# Check fine-tune progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fda90",
   "metadata": {
    "id": "cc7fda90"
   },
   "source": [
    "Check the progress and get a list of all the fine-tuning events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cf062ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1700666273921,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "4cf062ab",
    "outputId": "76fcef82-5ea9-40e8-a873-f3d0b8577a67",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuneEventsListResponse(data=[FineTuneEvent(created_at=1701381173, level='info', message='Created fine-tune: ft-DP03u7KFRxaaWEcFUPqTKaaq', object='fine-tune-event')], object='list')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_events = client.fine_tunes.list_events(fine_tune_id=fine_tune_response.id)\n",
    "fine_tune_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "HJc6iRazsU9N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1700666277357,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "HJc6iRazsU9N",
    "outputId": "34741692-cff9-45ee-ff78-55883adcecb1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FineTuneEvent(created_at=1701381173, level='info', message='Created fine-tune: ft-DP03u7KFRxaaWEcFUPqTKaaq', object='fine-tune-event')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_events.__dict__['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f5bf3",
   "metadata": {
    "id": "362f5bf3"
   },
   "source": [
    "Check the progress and get an object with the fine-tuning job data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f01831be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1700666896304,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "f01831be",
    "outputId": "24f8b1c4-699e-41d7-bce8-0a59eb7e0b03",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTune(id='ft-DP03u7KFRxaaWEcFUPqTKaaq', created_at=1701381173, fine_tuned_model=None, hyperparams=Hyperparams(batch_size=None, learning_rate_multiplier=None, n_epochs=4, prompt_loss_weight=0.01, classification_n_classes=None, classification_positive_class=None, compute_classification_metrics=None), model='curie', object='fine-tune', organization_id='org-heTRxyuue7iTIuT3uUuUFCPN', result_files=[], status='pending', training_files=[FileObject(id='file-kVYjbnur011YblP88xufg1Fu', bytes=244, created_at=1701381173, filename='training_data.jsonl', object='file', purpose='fine-tune', status='uploaded', status_details=None)], updated_at=1701381173, validation_files=[], events=[FineTuneEvent(created_at=1701381173, level='info', message='Created fine-tune: ft-DP03u7KFRxaaWEcFUPqTKaaq', object='fine-tune-event')])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_response = client.fine_tunes.retrieve(fine_tune_id=fine_tune_response.id)\n",
    "retrieve_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "FqW-k0EBs8DH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1700666283467,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "FqW-k0EBs8DH",
    "outputId": "0523eabc-fc8b-475f-a933-56cebaca39b6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'created_at', 'fine_tuned_model', 'hyperparams', 'model', 'object', 'organization_id', 'result_files', 'status', 'training_files', 'updated_at', 'validation_files', 'events'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'ft-DP03u7KFRxaaWEcFUPqTKaaq',\n",
       " 'created_at': 1701381173,\n",
       " 'fine_tuned_model': None,\n",
       " 'hyperparams': Hyperparams(batch_size=None, learning_rate_multiplier=None, n_epochs=4, prompt_loss_weight=0.01, classification_n_classes=None, classification_positive_class=None, compute_classification_metrics=None),\n",
       " 'model': 'curie',\n",
       " 'object': 'fine-tune',\n",
       " 'organization_id': 'org-heTRxyuue7iTIuT3uUuUFCPN',\n",
       " 'result_files': [],\n",
       " 'status': 'pending',\n",
       " 'training_files': [FileObject(id='file-kVYjbnur011YblP88xufg1Fu', bytes=244, created_at=1701381173, filename='training_data.jsonl', object='file', purpose='fine-tune', status='uploaded', status_details=None)],\n",
       " 'updated_at': 1701381173,\n",
       " 'validation_files': [],\n",
       " 'events': [FineTuneEvent(created_at=1701381173, level='info', message='Created fine-tune: ft-DP03u7KFRxaaWEcFUPqTKaaq', object='fine-tune-event')]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(retrieve_response.__dict__.keys())\n",
    "retrieve_response.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729581a",
   "metadata": {
    "id": "9729581a"
   },
   "source": [
    "**Wait for the model to be fine-tune, it takes 15/20 minutes!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e846ed",
   "metadata": {
    "id": "07e846ed"
   },
   "source": [
    "# Save fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635655e",
   "metadata": {
    "id": "3635655e"
   },
   "source": [
    "If `fine_tune_response.fine_tuned_model == None:` you can get the **fine_tuned_model** by listing all fine-tune events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77d2f317",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1700666287355,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "77d2f317",
    "outputId": "bf7d5efe-fa70-4cf5-ba85-c9a82cf7cbdc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curie:ft-austincapitaldata-2023-11-21-20-49-44\n"
     ]
    }
   ],
   "source": [
    "if fine_tune_response.fine_tuned_model == None:\n",
    "    #fine_tune_list = openai.FineTune.list()\n",
    "    fine_tune_list=client.fine_tunes.list()\n",
    "    fine_tune_dict=fine_tune_list.__dict__\n",
    "    fine_tuned_model = fine_tune_dict['data'][0].fine_tuned_model\n",
    "\n",
    "print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601df11",
   "metadata": {
    "id": "8601df11"
   },
   "source": [
    "# Test the new model on a new prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654268c",
   "metadata": {
    "id": "c654268c"
   },
   "source": [
    "Remember to end the prompt with the same suffix as we used in the training data; ` ->`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37cfb2da",
   "metadata": {
    "id": "37cfb2da",
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_prompt = \"Which country serves as the primary capital of Paris? ->\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bee69cb8",
   "metadata": {
    "id": "bee69cb8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = client.completions.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt,\n",
    "  max_tokens=10, # Change amount of tokens for longer completion\n",
    "  temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "RDq2cUhOyp4_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1700666295721,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "RDq2cUhOyp4_",
    "outputId": "f91b6bc5-33a9-444e-da1e-ad25f43a8804",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " France\n",
      "\n",
      "What is the capital of France?\n"
     ]
    }
   ],
   "source": [
    "#answer['choices'][0]['text']\n",
    "print(dict(answer.choices[0])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df769747-1842-4e17-bb85-3c5268087460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "colab": {
   "provenance": []
  },
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
