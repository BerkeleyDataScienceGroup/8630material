{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b0d16f",
   "metadata": {
    "id": "d5b0d16f"
   },
   "source": [
    "# Classification with OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e6fa9",
   "metadata": {
    "id": "d04e6fa9"
   },
   "source": [
    "Here's how you can fine-tune a GPT-3 model with Python using your own data for classification task.\n",
    "\n",
    "There is no real dataset provided for this task, but only sample of data and how it should be prepared for classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c47ab",
   "metadata": {
    "id": "d60c47ab"
   },
   "source": [
    "Install `openai` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7aacd7e",
   "metadata": {
    "id": "b7aacd7e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afa4af",
   "metadata": {
    "id": "d3afa4af"
   },
   "source": [
    "Import necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56483e72",
   "metadata": {
    "id": "56483e72",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f669e",
   "metadata": {
    "id": "698f669e"
   },
   "source": [
    "# Define OpenAI API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcaff120",
   "metadata": {
    "id": "dcaff120",
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_key =\"\"\n",
    "#openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cAwwFOEVoV7V",
   "metadata": {
    "id": "cAwwFOEVoV7V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f08dd7",
   "metadata": {
    "id": "10f08dd7"
   },
   "source": [
    "# Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f0c1f",
   "metadata": {
    "id": "569f0c1f"
   },
   "source": [
    "**PROMPT**\n",
    "\n",
    "According to the [OpenAI API reference](https://beta.openai.com/docs/guides/fine-tuning \"fine-tuning reference\") you need to make sure to end each `prompt` with a suffix.\n",
    "\n",
    "You can use ` ->`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6485dbd",
   "metadata": {
    "id": "a6485dbd"
   },
   "source": [
    "Template:\n",
    "\n",
    "```\n",
    "data_file = [{\n",
    "    \"prompt\": \"Prompt ->\",\n",
    "    \"completion\": \" Class\"\n",
    "},{\n",
    "    \"prompt\":\"Prompt ->\",\n",
    "    \"completion\": \" Class\"\n",
    "}]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5904b9",
   "metadata": {
    "id": "ca5904b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [{\n",
    "    \"prompt\": \"burger ->\",\n",
    "    \"completion\": \" edible\"\n",
    "},\n",
    "{\n",
    "    \"prompt\":\"paper towels ->\",\n",
    "    \"completion\": \" inedible\"\n",
    "},\n",
    "{\n",
    "    \"prompt\":\"vino ->\",\n",
    "    \"completion\": \" edible\"\n",
    "},\n",
    "{\n",
    "    \"prompt\":\"bananas ->\",\n",
    "    \"completion\": \" edible\"\n",
    "},\n",
    "{\n",
    "    \"prompt\":\"dog toy ->\",\n",
    "    \"completion\": \" inedible\"\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8024dd",
   "metadata": {
    "id": "df8024dd"
   },
   "source": [
    "During fine-tuning, the model reads the training examples and after each **token** of text, it predicts the next token. This predicted next token is compared with the actual next token, and the modelâ€™s internal weights are updated to make it more likely to predict correctly in the future. As training continues, the model learns to produce the patterns demonstrated in your training examples.\n",
    "\n",
    "You can check on this [OpenAI Tokenizer](https://platform.openai.com/tokenizer) how your token will be split (to get the number of tokens for your output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10cb45e",
   "metadata": {
    "id": "e10cb45e"
   },
   "source": [
    "# Save dict as JSONL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ed465",
   "metadata": {
    "id": "7c6ed465"
   },
   "source": [
    "Training data need to be a JSONL document.\n",
    "\n",
    "JSONL file is a newline-delimited JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4bc0cf",
   "metadata": {
    "id": "4e4bc0cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = \"training_data_classification.jsonl\"\n",
    "\n",
    "with open(file_name, 'w') as output_file:\n",
    "    for entry in data:\n",
    "        json.dump(entry, output_file)\n",
    "        output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf169e",
   "metadata": {
    "id": "6cbf169e"
   },
   "source": [
    "# Check JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a5fe452",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1974,
     "status": "ok",
     "timestamp": 1700667027504,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "7a5fe452",
    "outputId": "45fcaffa-d849-484d-f1f5-9a07b05d7f54",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Works in colab not in sagemaker\n",
    "!openai -k {api_key} tools fine_tunes.prepare_data -f training_data_classification.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fed3c",
   "metadata": {
    "id": "fe2fed3c"
   },
   "source": [
    "# Upload file to your OpenAI account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa6f8267",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1362,
     "status": "ok",
     "timestamp": 1700667117911,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "fa6f8267",
    "outputId": "00f1ac74-e04c-4ede-8117-6d2ac1f493ab",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-S3FlBuBNZYy75bzoXH3riJOo', bytes=255, created_at=1701409989, filename='training_data_classification.jsonl', object='file', purpose='fine-tune', status='uploaded', status_details=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_response = client.files.create(\n",
    "  file=open(file_name, \"rb\"),\n",
    " purpose='fine-tune'\n",
    ")\n",
    "upload_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb4254",
   "metadata": {
    "id": "20fb4254"
   },
   "source": [
    "# Save file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93469f37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1700667125289,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "93469f37",
    "outputId": "ca7be4cf-f80a-4110-e6b5-81654fd12f86",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-S3FlBuBNZYy75bzoXH3riJOo'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_id = upload_response.id\n",
    "file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1007a6",
   "metadata": {
    "id": "3e1007a6"
   },
   "source": [
    "# Fine-tune a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2da9b",
   "metadata": {
    "id": "57e2da9b"
   },
   "source": [
    "Every fine-tuning job starts from a base model, which defaults to **curie**.\n",
    "\n",
    "The choice of model influences both the performance of the model and the cost of running your fine-tuned model.\n",
    "\n",
    "Your model can be one of: **ada**, **babbage**, **curie**, or **davinci**.\n",
    "\n",
    "The default model is **Curie**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16bb42a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1700667764591,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "16bb42a2",
    "outputId": "8f54f4bd-e5d5-4596-9e7c-4ca9dad39676",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ft-3QBFVgcYAXbtcIzWcaDzL1vG',\n",
       " 'created_at': 1701409997,\n",
       " 'fine_tuned_model': None,\n",
       " 'hyperparams': Hyperparams(batch_size=None, learning_rate_multiplier=None, n_epochs=4, prompt_loss_weight=0.01, classification_n_classes=None, classification_positive_class=None, compute_classification_metrics=None),\n",
       " 'model': 'ada',\n",
       " 'object': 'fine-tune',\n",
       " 'organization_id': 'org-heTRxyuue7iTIuT3uUuUFCPN',\n",
       " 'result_files': [],\n",
       " 'status': 'pending',\n",
       " 'training_files': [FileObject(id='file-S3FlBuBNZYy75bzoXH3riJOo', bytes=255, created_at=1701409989, filename='training_data_classification.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)],\n",
       " 'updated_at': 1701409997,\n",
       " 'validation_files': [],\n",
       " 'events': [FineTuneEvent(created_at=1701409997, level='info', message='Created fine-tune: ft-3QBFVgcYAXbtcIzWcaDzL1vG', object='fine-tune-event')]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_response = client.fine_tunes.create(\n",
    "    training_file=file_id,\n",
    "    model=\"ada\")\n",
    "dict(fine_tune_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059b2b3",
   "metadata": {
    "id": "2059b2b3"
   },
   "source": [
    "# Check fine-tune progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fda90",
   "metadata": {
    "id": "cc7fda90"
   },
   "source": [
    "Check the progress and get a list of all the fine-tuning events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cf062ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1700667854140,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "4cf062ab",
    "outputId": "e0876d1c-d37b-4fb4-ab48-4a66020aafe1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [FineTuneEvent(created_at=1701409997, level='info', message='Created fine-tune: ft-3QBFVgcYAXbtcIzWcaDzL1vG', object='fine-tune-event')],\n",
       " 'object': 'list'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_events = client.fine_tunes.list_events(\n",
    "    fine_tune_id=fine_tune_response.id)\n",
    "dict(fine_tune_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f5bf3",
   "metadata": {
    "id": "362f5bf3"
   },
   "source": [
    "Check the progress and get an object with the fine-tuning job data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f01831be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1700667938941,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "f01831be",
    "outputId": "e962ebc7-15f2-45b7-bfa0-067de3239a1d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ft-3QBFVgcYAXbtcIzWcaDzL1vG',\n",
       " 'created_at': 1701409997,\n",
       " 'fine_tuned_model': None,\n",
       " 'hyperparams': Hyperparams(batch_size=1, learning_rate_multiplier=0.1, n_epochs=4, prompt_loss_weight=0.01, classification_n_classes=None, classification_positive_class=None, compute_classification_metrics=None),\n",
       " 'model': 'ada',\n",
       " 'object': 'fine-tune',\n",
       " 'organization_id': 'org-heTRxyuue7iTIuT3uUuUFCPN',\n",
       " 'result_files': [],\n",
       " 'status': 'running',\n",
       " 'training_files': [FileObject(id='file-S3FlBuBNZYy75bzoXH3riJOo', bytes=255, created_at=1701409989, filename='training_data_classification.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)],\n",
       " 'updated_at': 1701410003,\n",
       " 'validation_files': [],\n",
       " 'events': [FineTuneEvent(created_at=1701409997, level='info', message='Created fine-tune: ft-3QBFVgcYAXbtcIzWcaDzL1vG', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410002, level='info', message='Fine-tune costs $0.00', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410002, level='info', message='Fine-tune enqueued. Queue number: 0', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410003, level='info', message='Fine-tune started', object='fine-tune-event')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve_response = openai.FineTune.retrieve(id=fine_tune_response.id)\n",
    "retrieve_response = client.fine_tunes.retrieve(\n",
    "    fine_tune_id=fine_tune_response.id\n",
    "    )\n",
    "dict(retrieve_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda8384",
   "metadata": {
    "id": "ddda8384"
   },
   "source": [
    "## NOTE: **Wait at least 15-20 minuts so the job can finish!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca86136",
   "metadata": {
    "id": "6ca86136"
   },
   "source": [
    "### Troubleshooting fine_tuned_model as null\n",
    "During the fine-tuning process, the **fine_tuned_model** key may not be immediately available in the fine_tune_response object returned by `openai.FineTune.create()`.\n",
    "\n",
    "To check the status of your fine-tuning process, you can call the `openai.FineTune.retrieve()` function and pass in the **fine_tune_response.id**. This function will return a JSON object with information about the training status, such as the current epoch, the current batch, the training loss, and the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e846ed",
   "metadata": {
    "id": "07e846ed"
   },
   "source": [
    "# Save fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62135e",
   "metadata": {
    "id": "0d62135e"
   },
   "source": [
    "Once the fine-tuning process is complete, you can retrieve the fine_tuned_model key by calling the openai.FineTune.retrieve() function again and passing in the fine_tune_response.id. This will return a JSON object with the key fine_tuned_model and the ID of the fine-tuned model that you can use for further completions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635655e",
   "metadata": {
    "id": "3635655e"
   },
   "source": [
    "Note: If `fine_tune_response.fine_tuned_model == None:` you can get the **fine_tuned_model** by listing all fine-tune events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f042d8de-6979-4c75-8e04-576297831d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [FineTuneEvent(created_at=1701409997, level='info', message='Created fine-tune: ft-3QBFVgcYAXbtcIzWcaDzL1vG', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410002, level='info', message='Fine-tune costs $0.00', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410002, level='info', message='Fine-tune enqueued. Queue number: 0', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410003, level='info', message='Fine-tune started', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410017, level='info', message='Completed epoch 1/4', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410018, level='info', message='Completed epoch 2/4', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410019, level='info', message='Completed epoch 3/4', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410019, level='info', message='Completed epoch 4/4', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410034, level='info', message='Uploaded model: ada:ft-austincapitaldata-2023-12-01-05-53-54', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410035, level='info', message='Uploaded result file: file-URSyuUSo27Y7Th22PuMU6imi', object='fine-tune-event'),\n",
       "  FineTuneEvent(created_at=1701410035, level='info', message='Fine-tune succeeded', object='fine-tune-event')],\n",
       " 'object': 'list'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tunes.list_events(fine_tune_id=fine_tune_response.id).__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a6e699-732c-4240-9e03-05698c87016a",
   "metadata": {},
   "source": [
    "# When this does not say none, you can run the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77d2f317",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1700668038235,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "77d2f317",
    "outputId": "cb4fa5fa-08bf-44b3-d3e9-b29a473fa899",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ada:ft-austincapitaldata-2023-12-01-05-53-54\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = client.fine_tunes.retrieve(\n",
    "    fine_tune_id=fine_tune_response.id)\\\n",
    "    .fine_tuned_model\n",
    "\n",
    "#fine tuned model it\n",
    "print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601df11",
   "metadata": {
    "id": "8601df11"
   },
   "source": [
    "# Test the new model on a new prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654268c",
   "metadata": {
    "id": "c654268c"
   },
   "source": [
    "Remember to end the prompt with the same suffix as we used in the training data; ` ->`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38b7ff83",
   "metadata": {
    "id": "38b7ff83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "def get_class(completion_text):\n",
    "    if completion_text == ' edible':\n",
    "        label = 'edible'\n",
    "    elif completion_text == ' in':\n",
    "        label = 'inedible'\n",
    "    else:\n",
    "        label = 'other'\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b0238",
   "metadata": {
    "id": "0a4b0238",
    "tags": []
   },
   "source": [
    "## temperature and max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcbcf18",
   "metadata": {
    "id": "abcbcf18"
   },
   "source": [
    "TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bee69cb8",
   "metadata": {
    "id": "bee69cb8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other\n"
     ]
    }
   ],
   "source": [
    "new_prompt = \"table ->\"\n",
    "\n",
    "api_response = client.completions.create(\n",
    "    model=fine_tuned_model,\n",
    "    prompt=new_prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=1\n",
    ")\n",
    "\n",
    "completion_text = api_response.choices[0].text\n",
    "get_class(completion_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d617b518-3207-4d74-be37-ae5b1a44a1b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other\n"
     ]
    }
   ],
   "source": [
    "new_prompt = \"printer paper ->\"\n",
    "\n",
    "api_response = client.completions.create(\n",
    "    model=fine_tuned_model,\n",
    "    prompt=new_prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=1\n",
    ")\n",
    "\n",
    "completion_text = api_response.choices[0].text\n",
    "get_class(completion_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd20d5",
   "metadata": {
    "id": "7ccd20d5"
   },
   "source": [
    "TEST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32458c6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1700669718747,
     "user": {
      "displayName": "Gunnar Kleemann",
      "userId": "02122813220075330457"
     },
     "user_tz": 360
    },
    "id": "32458c6d",
    "outputId": "a2e125b5-f9ee-40bf-dec2-6eff06e6530f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' edible')]\n",
      "edible\n"
     ]
    }
   ],
   "source": [
    "new_prompt = \"steak ->\"\n",
    "\n",
    "api_response = client.completions.create(\n",
    "    model=fine_tuned_model,\n",
    "    prompt=new_prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=1\n",
    ")\n",
    "\n",
    "\n",
    "#completion_text = api_response['choices'][0]['text']\n",
    "completion_text = api_response.choices[0].text\n",
    "print(api_response.choices)\n",
    "get_class(completion_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "35447a8b-bc23-4856-877d-dc4c1a2aee10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' 1')]\n",
      "other\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae10697-2630-458b-9ef7-f27789b91ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "colab": {
   "provenance": []
  },
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
